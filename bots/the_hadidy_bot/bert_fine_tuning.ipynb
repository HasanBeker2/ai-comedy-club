{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jokes_df = pd.read_csv('/kaggle/input/jester-17m-jokes-ratings-dataset/jester_items.csv')\nratings_df = pd.read_csv('/kaggle/input/jester-17m-jokes-ratings-dataset/jester_ratings.csv')\nunique_categories = jokes_df['jokeId'].unique()\n\nprint(len(unique_categories))\nprint(len(jokes_df))\nprint(len(ratings_df))\n\nprint(jokes_df.head())\nprint(ratings_df.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"most_common_user_id = ratings_df['userId'].value_counts().idxmax()\nprint(most_common_user_id)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ratings_df = ratings_df.loc[ratings_df['userId'] == most_common_user_id]\n\ndataset_df = pd.merge(jokes_df, ratings_df, on='jokeId', how='inner')\n#dataset_df = dataset_df[ ['jokeText','rating','jokeId'] ]\nunique= dataset_df['jokeId'].unique()\nprint(len(unique))\nprint(dataset_df.head())\nprint(len(dataset_df))\n\navg_rating_df = dataset_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_df = data\naverage_ratings = dataset_df.groupby('jokeId')['rating'].mean()\nprint(len(average_ratings))\naverage_ratings.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#avg_rating_df = pd.DataFrame({'jokeId': average_ratings.index, 'avg_rating': average_ratings.values})\navg_rating_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.hist(avg_rating_df['rating'], bins=10, edgecolor='black')\n\nplt.title('Histogram of Average Ratings')\nplt.xlabel('Rating')\nplt.ylabel('Frequency')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original_min = avg_rating_df['rating'].min()\noriginal_max = avg_rating_df['rating'].max()\n\nprint(original_min,original_max)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target_min = 0\ntarget_max = 10\n\n# Apply the linear mapping formula\navg_rating_df['rating'] = ((avg_rating_df['rating'] - original_min) / (original_max - original_min)) * (target_max - target_min) + target_min\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.hist(avg_rating_df['rating'], bins=10, edgecolor='black')\n\nplt.title('Histogram of Ratings')\nplt.xlabel('Rating')\nplt.ylabel('Frequency')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#avg_rating_df = pd.merge(avg_rating_df, jokes_df, on='jokeId', how='inner')\n#print(len(avg_rating_df))\navg_rating_df.head()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_rating_df['rating'] = avg_rating_df['rating'].round().astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"avg_rating_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.hist(avg_rating_df['rating'], bins=10, edgecolor='black')\n\nplt.title('Histogram of Average Ratings')\nplt.xlabel('Average Rating')\nplt.ylabel('Frequency')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Filter jokes with an average rating of 10\nhigh_avg_rating_jokes = avg_rating_df[avg_rating_df['avg_rating'] == 10]\n\n# Calculate the number of jokes to delete (25% of high-average-rating jokes)\nnum_jokes_to_delete = int(len(high_avg_rating_jokes) * 0.35)\n\n# Randomly select jokes to delete\njokes_to_delete = high_avg_rating_jokes.sample(n=num_jokes_to_delete, random_state=42)\n\n# Remove the selected jokes from the DataFrame\navg_rating_df = avg_rating_df.drop(jokes_to_delete.index)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.hist(avg_rating_df['avg_rating'], bins=10, edgecolor='black')\n\nplt.title('Histogram of Average Ratings')\nplt.xlabel('Average Rating')\nplt.ylabel('Frequency')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data, val_data = train_test_split(avg_rating_df, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load BERT tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=11)  # 11 classes (0 to 10)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence  # Import pad_sequence\n\ndef tokenize_and_pad(texts):\n    tokenized_texts = [tokenizer.encode(text, add_special_tokens=True) for text in texts]\n    padded_texts = pad_sequence([torch.tensor(tokens) for tokens in tokenized_texts], batch_first=True)\n    return padded_texts\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_inputs = tokenize_and_pad(train_data['jokeText'])\ntrain_labels = torch.tensor(train_data['rating'], dtype=torch.long)\ntrain_dataset = TensorDataset(train_inputs, train_labels)\ntrain_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define optimizer and loss function\noptimizer = AdamW(model.classifier.parameters(), lr=1e-5, weight_decay=0.01)\ncriterion = torch.nn.CrossEntropyLoss()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 50\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n\n    for batch in tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n        optimizer.zero_grad()\n        inputs, labels = batch\n        inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to GPU\n        outputs = model(inputs)[0]\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_dataloader)}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_joke_class(joke_text):\n    # Tokenize and prepare the input for the model\n    tokenized_text = tokenizer.encode(joke_text, add_special_tokens=True)\n    padded_text = pad_sequence([torch.tensor(tokenized_text)], batch_first=True).to(device)\n    model.eval()\n    # Get the predicted class\n    with torch.no_grad():\n        outputs = model(padded_text)[0]\n        predicted_class = torch.argmax(outputs).item()\n    \n    return predicted_class\n\n# Example usage\njoke = \" He'll stop at nothing to avoid them.\"\npredicted_class = get_joke_class(joke)\nprint(f'Predicted Class: {predicted_class}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\ntrue_classes = val_data['rating'].tolist()\npredicted_classes = [get_joke_class(joke_text) for joke_text in val_data['jokeText']]\nf1 = f1_score(true_classes, predicted_classes, average='macro')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save_pretrained('trained_classification_model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}